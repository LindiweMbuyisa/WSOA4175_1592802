<!DOCTYPE html>
<html>
<head>
	<title>BlogPost_12</title>
	<meta charset="utf-8">
	 <meta name="viewport" content="width=device-width, initial-scale=1.0">
	 <link rel="stylesheet" type="text/css" href="CSS/stylesheet.css">
	 <script src ="JS/myScript.js" type="text/javascript"></script>

</head>
<body>

	<nav> 
		<div class="logo" style="float: left;"><a href="index.html">Lindiwe:)</a></div>
		<div class="hamburger-menu">
		<div class="bar"></div>
		<div class="bar"></div>
		<div class="bar"></div>
		</div>
		<ul class="links">
		<li><a href="Creative.html">Creative Projects</a>
		<li class="current-link"><a href="Blog.html">Blog</a></li>
		<li><a href="About.html">About</a></li>
		</li>
	</ul>
	</nav>
	<div>
	<button onclick="Back()">Back</button>
	
	</div>
	<section>
		<h1 class="title">"Criminal Machine Learning"</h1>
		<p class="subtitle">This blog post highlights Bergstrom and West's criticism of a criminal machine learning case study incorporating scholars like Boulamwini and O'Neil's ideas focusing on algorithmic bias in machine learning and AI.</p>
		<p class="date">25 September 2020</p>
		<p class="paragraph"> In reference to research conducted by Zhang and Wu which explored the use of machine learning to detect features of the human being’s face that could show signs/links to criminality; They put forward that algorithms/machine learning can be used to tell if a person is a criminal or basically based on a simple headshot the machine can point out if a person  is a criminal and accurately. Bergstrom and West (2019) suggest that this poses and comes with a lot of ethical implications. The question of how we can identify people/individuals as criminals before they have even acted/even committed the crime. This study similar to Lombroso’s physiognomic criminology which was a scientific theory of criminality. His theory was that criminals were born as such that they exhibit both psychological drives, physical features. His ideas: theory suggested that the shape of the jaw, the slope of the forehead, the size of the eyes, the structure of the ear all contained significant clues or meant something about the individual’s morality. The Wu and Zhang’s approach which stems from Lombroso’s research/program, the researcher’s aimed to show that advanced machine learning to image processing has the ability to show clues, patterns that Lombroso suggest. The research proved that 90% of the time the algorithm is accurate and these algorithm were free from any of the biases, prejudices that come with human judgement. Bergstrom and West(2019)  suggest that looking at the data sets to train the algorithm is very much problematic because they have selected specific images, this means they are measuring their accuracy based on the the specific images they have selected for the study. That in its self is a bias. Like Cathy O’Neil points out that algorithms are like opinions embedded in code (Ted Talk).
		Bias is injected into algorithms based on the data we collect, for example the police system in America is embedded with systemic racism and this means that black have a high chance of being identified as a criminal in comparison to the white other races. They also have a higher chance of being arrested, also receiving a high sentence even they have committed the very same exact crime in comaprison to other races. Imagine if there was a machine learning identifying the criminals and sentencing in America policing system. 
		With the research, the data collected was only 1800 photo of Chinese Men aged between 18-55, no facial hair, scars, tattoos. 1100 of photo identified as non-criminals. This suggest  that  the algorithm could be learning the different ways(facial features) to make one convictable as opposed to identifying as a criminal (Bergstrom & West 2019). As the individuals that are more likely to be identified as guilty are mostly unattractive. With the research conducted by Zu and Whang; with the photos that were ID photos they were mainly identified as criminals as opposed to pictures/images that were posted for promotional purposes, as these were profile pictures which the individual had 100% choice when taking the image.  Bergstrom and West(2019) suggest that the criminal and non-criminal faces are identified by their facial features; they have pointed out that has identified that the shorter the distance between the upper nose section; and the inner corners of the eyes, this suggests that they are a criminal. Bergstrom and West suggest that with the images of the criminals that are not smiling whilst that is the opposite for the individuals that are identified as not criminals. Bergstrom and West point out that there is a major difference facial features with facial expressions. Bergstrom and West(2019) suggest that extraordinary claims require solid evidence. Even though they(Zu and Whang) have justified that the research had good intentions.  
 		</p>
		<p class ="paragraph">This is similar to Heilweil’s (2020) argument  which points out that machine learning is based on systems that are trained on data with the computer learning to use that data to make judgement, predictions, this is all based on the information it processes based on the patterns it notices /picks up from the data. These systems that are built are biased depending on who builds them, how they are developed and how they are used. A similar example; is when google images used machine learning to detect/label the faces of individuals; detecting the faces of black people, it labelled them as ‘monkeys’. Boulamwini – a scholar who advocates algorithm bias and the ‘coded gaze’ points to having her face not being detected by a AI system/ machine until she covered her face with a white mask. Boulamwini ‘experience of algorithmic bias might not be destructive but when we were to use these systems for deciding employment, insurance, deciding who gets a job, loan or education. They might create discriminative, unethical practices and experiences. She highlights that in order to fight and change the algorithmic bias, we need to raise awareness about our existing bias, the societal impacts of AI in existing data centric technology. Boulamwini further suggests that we need to mitigate bias – diversify data and develop inclusive practices for the design, development, deployment testing of AI and inclusive practices. Boulamwini further points out that technology should not be for specific people but should be for all of us; not discriminative to a certain group but it is about having a full spectrum of representation. It is about asking the questions who codes? 
		I think we could change, check our biases, raise awareness of the bias but it is important to point out that we are allowing algorithms to decide out fate. But also what happens when that machine makes a mistake, how do you tell it to retract its decision/ to check if it has not made a mistake like we would do with a person. </p>
	</section>
	<section>
		
		<h2>References</h2>
		<h3> Bergstrom & West. 2019. Criminal Machine Learning.</h3>
		<h3>Ted Talk, 2017. The era of blind faith in big data must end. Available at: <cite><a href="https://www.ted.com/talks/cathy_o_neil_the_era_of_blind_faith_in_big_data_must_end?language=en">The era of blind faith in big data must end</a></cite></h3>
		<h3>Ted Talk, 2016. How I'm fighting bias in algorithms. Available at: <cite><a href="https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms?language=en"> How I'm fighting algorithmic bias</a></cite></h3>
		<h3>Heilweil,R. 2020. Vox. Why algorithms can be racist and sexist. Accessed from the World Wide Web:22 September 2020.<cite><a href="https://www.vox.com/recode/2020/2/18/21121286/algorithms-bias-discrimination-facial-recognition-transparency">Algorithms can be racist and sexist.</a></cite></h3>
	</section>
	<script src ="JS/nav.js" type="text/javascript"></script>
	
</body>
</html>