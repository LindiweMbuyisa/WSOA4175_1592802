<!DOCTYPE html>
<html>
<head>
	<title>BlogPost_12</title>
	<meta charset="utf-8">
	 <meta name="viewport" content="width=device-width, initial-scale=1.0">
	 <link rel="stylesheet" type="text/css" href="CSS/stylesheet.css">
	 <script src ="JS/myScript.js" type="text/javascript"></script>

</head>
<body>

	<nav> 
		<div class="logo" style="float: left;"><a href="index.html">Lindiwe:)</a></div>
		<div class="hamburger-menu">
		<div class="bar"></div>
		<div class="bar"></div>
		<div class="bar"></div>
		</div>
		<ul class="links">
		
		<li class="current-link"><a href="Blog.html">Blog</a></li>
		<li><a href="About.html">About</a></li>
	
	</ul>
	</nav>
	
	<button onclick="Back()">Back</button>
	

	<article>
		<h1 class="title">"Criminal Machine Learning"</h1>
		<p class="date">25 September 2020</p>
		<p class="paragraph"> Zhang and Wu's research explored the use of machine learning to detect features of the human face that could show signs/links to criminality. They argued that algorithms/machine learning can be used to tell if a person is a criminal based on a simple headshot the machine can point out if a person  is a criminal and not. Bergstrom and West (2019) suggest that this is problematic and comes with a lot of ethical implications. They question how can they identify people/individuals as criminals before they have not committed a crime. This study is similar to Lombroso’s physiognomic criminology which was a scientific theory of criminality. His theory was that criminals were born as such that they exhibit both psychological drives and physical features. His theory suggested that the shape of the jaw, the slope of the forehead, the size of the eyes and the structure of the ear all contained significant clues or meant something about the individual’s morality. Zhang and Wu's research aimed to show that advanced machine learning to image processing has the ability to show clues, patterns that Lombroso suggest in his theory. The research proved that 90% of the time the algorithm is accurate and these algorithm were free from any of the biases and  prejudices that come with human judgement. Bergstrom and West(2019)  suggest that looking at the data sets to train the algorithm is very much problematic because they have selected specific images, this means they are measuring their accuracy based on the specific images they have selected for the study and that in its self is a bias. Like Cathy O’Neil points out that algorithms are like opinions embedded in code (Cathy O'neil 2017).
		Bias is injected into algorithms based on the data we collect, for example the police system in America is embedded with systemic racism and this means that black people have a high chance of being identified as a criminal in comparison to other races. They also have a higher chance of being arrested, also receiving a high sentence even if they have committed the same crime in comparison to other races. 
		With the research, the data collected was only 1800 photos of Chinese Men aged between 18-55, no facial hair, scars, tattoos. 1100 of the photos were identified as non-criminals. This suggests that the algorithm could be learning the different ways(facial features) to make one convictable as opposed to identifying as a criminal. As the individuals that are more likely to be identified as guilty are mostly unattractive. With the research conducted by Zu and Whang, they collected photos that were ID photos and these mainly were identified as criminals as opposed to pictures/images that were posted for promotional purposes, as these were profile pictures which the individual had 100% choice.  What the authors point out is that the algorithm they are building is a form of bias. Bergstrom and West(2019) suggest that the criminal and non-criminal faces are identified by their facial features which means that based on the individuals's upper nose and inner corners of the eyes, this would suggest that they were a criminal or not. The shorter the distance between the upper nose section and the inner corners of the eyes, this suggests that they are a criminal. Bergstrom and West further suggest that with the images of the criminals that are not smiling in their photos were often identified as being a criminal. Bergstrom and West point out that there is a major difference in facial features with facial expressions. With regards to the results from the Wu and Zhang’s research, Bergstrom and West suggest that extraordinary claims require evidence. Even though they have justified that the research had good intentions, it is not enough to prove that a person is a criminal based on their facial features.  
 		</p>
		<p class="paragraph">
		Heilweil argues that machine learning is based on systems that are trained on data with the computer learning to use that data to make judgement and predictions, this is all based on the information it processes based on the patterns it notices /picks up from the data set provided. These systems that are built are biased depending on who builds them, how they are developed and how they are used. For example, when google images used machine learning to detect/label the faces of individuals, they detected the faces of black people and labelled them as ‘monkeys’. Boulamwini – a scholar who advocates for algorithm bias and the ‘coded gaze’ points out that she had difficulty trying to have her face detected by an AI system/machine and she had to cover her face with a white mask in order to have her face detected. Boulamwini’s experience of algorithmic bias might not be destructive but when we were to use these systems to decide who gets hired, loan or education. They might create discriminative, unethical practices and experiences. Taking into consideration that the world is unequal, discriminative and these can be perpetuated. She highlights that to fight and change the algorithmic bias, we need to raise awareness about our existing bias, the societal impacts of AI in existing data-centric technology. Boulamwini further suggests that we need to mitigate bias, diversify data and develop inclusive practises for the design, development and deployment testing of AI (Boulamwini 2016). This means that we need a spectrum of data not just selected data in order to create inclusive technology. Boulamwini further points out that technology should not be for specific people but should be for all of us. This should not be discriminative to a certain group, but it is about having a full spectrum of representation. It is about asking the questions who codes?. I think this could change the systems that we create. 
		 </p>
	
		
		<h2>References</h2>
		<h3> Bergstrom & West. 2019. Criminal Machine Learning.</h3>
		<h3>Ted Talk, 2017. The era of blind faith in big data must end. Available at: <cite><a href="https://www.ted.com/talks/cathy_o_neil_the_era_of_blind_faith_in_big_data_must_end?language=en">The era of blind faith in big data must end</a></cite></h3>
		<h3>Ted Talk, 2016. How I'm fighting bias in algorithms. Available at: <cite><a href="https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms?language=en"> How I'm fighting algorithmic bias</a></cite></h3>
		<h3>Heilweil,R. 2020. Vox. Why algorithms can be racist and sexist. Accessed from the World Wide Web:22 September 2020.<cite><a href="https://www.vox.com/recode/2020/2/18/21121286/algorithms-bias-discrimination-facial-recognition-transparency">Algorithms can be racist and sexist.</a></cite></h3>
	</article>
	<a href="Eleven.html" class="prevbtn btn">Prev</a>
	<a href="One.html" class="nextbtn btn">Next</a>
	<script src ="JS/nav.js" type="text/javascript"></script>
	
</body>
</html>